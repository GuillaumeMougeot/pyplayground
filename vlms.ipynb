{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d553a95",
   "metadata": {},
   "source": [
    "# Qwen2.5-VL and Qwen3\n",
    "\n",
    "Source: https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40937532-60f2-4c26-8452-198d2fc71011",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers qwen_vl_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebbcbbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from transformers import Qwen3VLMoeForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "from os import listdir \n",
    "from os.path import join \n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import cv2\n",
    "from pathlib import PureWindowsPath, Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28be40be",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Qwen2_5_VLForConditionalGeneration' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# default: Load the model on the available device(s)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#     \"Qwen/Qwen2.5-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m      5\u001b[39m \n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m model = \u001b[43mQwen2_5_VLForConditionalGeneration\u001b[49m.from_pretrained(\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mQwen/Qwen2.5-VL-3B-Instruct\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m     torch_dtype=torch.bfloat16,\n\u001b[32m     10\u001b[39m     attn_implementation=\u001b[33m\"\u001b[39m\u001b[33mflash_attention_2\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m     device_map=\u001b[33m\"\u001b[39m\u001b[33mcuda:0\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# default processer\u001b[39;00m\n\u001b[32m     15\u001b[39m processor = AutoProcessor.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mQwen/Qwen2.5-VL-3B-Instruct\u001b[39m\u001b[33m\"\u001b[39m, use_fast=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'Qwen2_5_VLForConditionalGeneration' is not defined"
     ]
    }
   ],
   "source": [
    "# default: Load the model on the available device(s)\n",
    "# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2.5-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    "# )\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"cuda:0\",\n",
    ")\n",
    "\n",
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccfec771-7f9c-4b87-9023-380111df9e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d70ad3e833d4497ab786b82636b9f70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# model = Qwen3VLMoeForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen3-VL-30B-A3B-Instruct\", dtype=\"auto\", device_map=\"auto\"\n",
    "# )\n",
    "model = Qwen3VLMoeForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen3-VL-30B-A3B-Instruct\",\n",
    "    dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "512a5cd9-d9aa-471a-9952-c845aa55842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-30B-A3B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7865ee99-fd55-48b6-9061-54f298ba3d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this video.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# inputs = {k:v.cuda() for k,v in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef116a0c-b099-4e4c-8f8c-910a53818874",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(inputs.to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9965fa8-d90c-435e-94b3-f704dddb7c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs.to('cuda'), max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfa0be8",
   "metadata": {},
   "source": [
    "Application to one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f891b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"file://data/scan.jpg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Transcribe the handwritten text in this picture.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "content_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    contents=content_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ad0921",
   "metadata": {},
   "source": [
    "Application to a batch of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af32ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handwritten Text Recognition\n",
    "\n",
    "# content_folder = \"data/insect\"\n",
    "# system_prompt = (\"You are a classification model. \"\n",
    "#                 \"You only gives two possible outputs: \"\n",
    "#                 \"'Yes' if the input content contains an insect visiting the \"\n",
    "#                 \"flower. 'No' if the input image does not contain an insect \"\n",
    "#                 \"or if the insect in the image is not on the flower. \"\n",
    "#                 \"Be careful: say 'No' if the insect is not ON the flower \"\n",
    "#                 \"but next to it. Say 'Yes' if the insect is hiding behind the flower. Some insect are small and hides.\")\n",
    "\n",
    "content_folder = \"data/20250613_htr_2\"\n",
    "note_taker = \"\"\"\n",
    "Transcribe handwritten text from scanned pages with precision and fidelity.\n",
    "\n",
    "Rules:\n",
    "- Preserve paragraph breaks, indentation, headers, footnotes, and notes.\n",
    "- Keep line breaks and spacing, unless a line break interrupts a sentence—then continue the line.\n",
    "- Copy punctuation, spelling, and capitalization exactly, even if archaic or nonstandard.\n",
    "- If a circled number appears (e.g. ①, ②), always replace it with plain text in this format: (1), (2), etc. Never output circled or Unicode numbers.\n",
    "- Preserve list structures and numbering as written, adjusting to (1), (2)... if needed.\n",
    "- Use plain text only—no markdown, symbols, or styling.\n",
    "- Mark unclear areas as [illegible], [unclear], or [guess].\n",
    "- Omit crossed-out text.\n",
    "- Use LaTeX-style for math (e.g., x_{{y}}).\n",
    "- Ignore non-handwritten content like logos or form lines.\n",
    "- Do not rewrite or correct the original—transcribe exactly what is written.\n",
    "\"\"\"\n",
    "\n",
    "reformatting_system=\"\"\"\n",
    "Reformat raw notes into clean plain text.\n",
    "\n",
    "Rules:\n",
    "- Standardize all numbered items to one consistent format: (1), (2), etc.\n",
    "- Replace circled numbers (e.g. ①) with (1), (2), etc. Keep them in order.\n",
    "- Do not use markdown or Unicode symbols. No bullets or asterisks.\n",
    "- Merge broken lines when a sentence clearly continues.\n",
    "- Keep paragraph breaks where the topic or comment shifts.\n",
    "- Retain all original wording. Do not rewrite or correct content.\n",
    "- Preserve math and variable notation as-is (e.g. θ^rpn, λ₀ = 1).\n",
    "- Output plain text only. No markdown syntax or special formatting.\n",
    "\"\"\"\n",
    "\n",
    "note_taker_md = \"\"\"\n",
    "You transcribe handwritten notes into structured Markdown format with high fidelity.\n",
    "\n",
    "Rules:\n",
    "- Preserve all original content exactly, including spelling, punctuation, and spacing, even if nonstandard or archaic.\n",
    "- Use Markdown headings for section titles if clearly indicated (e.g. underlined or larger handwriting).\n",
    "- Use Markdown numbered lists for itemized or circled numbers (① → 1. etc.), keeping the order accurate.\n",
    "- For inline math or equations, use LaTeX formatting wrapped in `$...$` or `$$...$$` if on its own line.\n",
    "- Mark illegible areas as **[illegible]** or **[unclear]**. Use **[guess]** for best-guess transcriptions.\n",
    "- Ignore crossed-out or struck-through text.\n",
    "- Do not include printed background elements (e.g. lines, logos, templates).\n",
    "- Do not wrap anything in backticks or asterisks unless clearly present in the handwriting.\n",
    "- Avoid interpreting or rephrasing — transcribe faithfully.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "reformatting_system_md=\"\"\"\n",
    "Reformat transcribed Markdown notes into clean, structured Markdown for clarity and consistency.\n",
    "\n",
    "Rules:\n",
    "- Ensure all numbered items use standard Markdown format (`1.`, `2.`, etc.).\n",
    "- Standardize any mixed formats (e.g. circled numbers, `(1)`, or inconsistent numbering) into clean Markdown lists.\n",
    "- Use headings (`##`, `###`, etc.) to organize sections logically based on topic shifts or section titles.\n",
    "- Merge broken lines within sentences; preserve breaks between ideas, questions, or sections.\n",
    "- Keep all math in LaTeX syntax (e.g. `$x_{{y}}$`), and preserve variable notation.\n",
    "- Bold or italicize for clarity only if meaning is implied (e.g. emphasis or notes), but do not invent formatting.\n",
    "- Mark illegible or unclear text clearly as `[illegible]`, `[unclear]`, or `[guess]`.\n",
    "- Output must remain plain Markdown — no HTML, no rendered output, no backtick blocks unless content is code.\n",
    "\"\"\"\n",
    "\n",
    "# Mice detector\n",
    "content_folder = \"data/test/mice/images\"\n",
    "system_prompt=\"\"\"\n",
    "You are a wildlife ecologist specialized in rodent and small mammal identification from images.\n",
    "\n",
    "Your task is to analyze the image and determine if it contains an animal.\n",
    "If an animal is present, classify it into one of the predefined categories.\n",
    "\n",
    "Output strictly one of the following labels as a plain string (no explanation, no formatting):\n",
    "\n",
    "labels = [\n",
    "    \"no_animal\",\n",
    "    \"mouse\",\n",
    "    \"vole\",\n",
    "    \"mustelid\",\n",
    "    \"shrew\",\n",
    "    \"rat\",\n",
    "    \"hedgehog\",\n",
    "    \"bird\",\n",
    "    \"ungulate\",\n",
    "    \"other_animal\"\n",
    "]\n",
    "\n",
    "If uncertain, choose the most likely category.\n",
    "Output nothing except the label.\n",
    "\"\"\"\n",
    "content_type=\"image\"\n",
    "\n",
    "# Wildlife camera detector\n",
    "# content_folder = \"/home/george/codes/pyplayground/data/test/wildcam/Klelund_0113_211011_211124\"\n",
    "# content_folder = \"/home/george/codes/pyplayground/data/test/wildcam/Klelund_0244_220530_220616\"\n",
    "# content_folder = \"/home/george/codes/pyplayground/data/test/wildcam/Tofte_0052_230524_230531\"\n",
    "content_folder = \"/work/datasets/wildcams/Tofte_0052_230524_230531\"\n",
    "# content_folder = \"/work/datasets/wildcams/Klelund_0113_211011_211124\"\n",
    "# content_folder = \"/work/datasets/wildcams/Klelund_0244_220530_220616\"\n",
    "# system_prompt_v1=\"\"\"\n",
    "# You are an automated wildlife observation analysis system. Your task is to analyze the provided video and extract key information into a single, specific CSV-formatted line.\n",
    "\n",
    "# **Instructions:**\n",
    "# 1.  Analyze the entire video for animal presence, behavior, and environmental context.\n",
    "# 2.  Extract the requested metadata from the text overlay.\n",
    "# 3.  Your entire output must be a single line of text containing 6 comma-separated values in the exact order specified below.\n",
    "# 4.  Do not output headers, explanations, or any text other than the single CSV line.\n",
    "# 5.  If a value cannot be determined, use the specified default.\n",
    "\n",
    "# **CSV Field Order and Content:**\n",
    "# 1.  **`animal_present`**: `TRUE` if an animal is visible, otherwise `FALSE`.\n",
    "# 2.  **`species_common_name`**: Common name. Default: `N/A`.\n",
    "# 3.  **`species_scientific_name`**: Scientific name. Default: `N/A`.\n",
    "# 4.  **`individual_count`**: Integer count of the primary species. Default: `0`.\n",
    "# 5.  **`demographic_composition`**: `Female`, `Male`, `Juvenile` or `Unknown`. Default: `N/A`.\n",
    "# 6.  **`behavior_description`**: Short, neutral keywords (e.g., `walking, foraging, observing`). Default: `N/A`.\n",
    "# \"\"\"\n",
    "# system_prompt_v2=\"\"\"\n",
    "# You are an expert wildlife analyst. Your task is to analyze the provided video and generate a single line of CSV data describing the primary animal species present.\n",
    "\n",
    "# Use the following header and rules for each field:\n",
    "# `species_common_name,species_scientific_name,individual_count,demographic_composition,behavior_description`\n",
    "\n",
    "# 1.  `species_common_name`: The common name of the primary species (e.g., `red deer`).\n",
    "# 2.  `species_scientific_name`: The scientific name of the primary species. One of: \n",
    "# 3.  `individual_count`: The total number of individuals of that species.\n",
    "# 4.  `demographic_composition`: Describe the primary subject or group. Use one: `male`, `female`, `juvenile`, `mixed group`, `unknown`.\n",
    "# 5.  `behavior_description`: Provide 1-3 comma-separated keywords describing the behavior (e.g., `\"walking,observing\"`).\n",
    "\n",
    "# If no animal is visible in the video, output the following default values:\n",
    "# `N/A,N/A,0,N/A,N/A`\n",
    "\n",
    "# Here is an example of valid output:\n",
    "# red deer,cervus elaphus,1,female,\"walking,observing\"\n",
    "# \"\"\"\n",
    "# system_prompt=\"\"\"\n",
    "# You are an expert wildlife analyst. Your task is to analyze the provided video and generate a single line of CSV data describing the primary animal species present.\n",
    "\n",
    "# Use the following header and rules for each field:\n",
    "# `species_common_name,individual_count,demographic_composition`\n",
    "\n",
    "# 1.  `species_common_name`: The common name of the primary species (e.g., `red deer`). One of: hare, red deer, roe deer, fox, boar, badger, wolf, cat, raccoon dog, marten sp., martes martes, mustelid sp., otter, polecat, squirrel, martes foina, fallow deer, hedgehog, carnivore sp., bird, unidentified mammal, other agents (explain in comments), dog, vehicle, ungulate sp.\n",
    "# 2.  `individual_count`: The total number of individuals of that species.\n",
    "# 3.  `demographic_composition`: Describe the primary subject or group. Use one: `male`, `female`, `juvenile`, `mixed group`, `unknown`.\n",
    "\n",
    "# If no animal is visible in the video, output the following default values:\n",
    "# `N/A,0,N/A`\n",
    "\n",
    "# Here is an example of valid output:\n",
    "# red deer,1,female\n",
    "# \"\"\"\n",
    "system_prompt=\"\"\"\n",
    "You are an expert wildlife analyst. Your task is to analyze the provided video and generate a single line of CSV data describing the primary animal species present. \n",
    "\n",
    "Use the following header and rules for each field:\n",
    "`species_common_name,individual_count,demographic_composition`\n",
    "\n",
    "1.  `species_common_name`: The common name of the primary species (e.g., `red deer`). If a human or a vehicle is present in the video, put 'human' or 'vehicle'\n",
    "2.  `individual_count`: The total number of individuals of that species.\n",
    "3.  `demographic_composition`: Describe the primary subject or group. Use one: `male`, `female`, `juvenile`, `mixed group`, `unknown`.\n",
    "\n",
    "If no animal and no vehicle and no human is visible in the video, output the following default values:\n",
    "`N/A,0,N/A`\n",
    "\n",
    "Here is an example of valid output:\n",
    "red deer,1,female\n",
    "\"\"\"\n",
    "content_type=\"video\"\n",
    "# header=\"species_common_name,species_scientific_name,individual_count,demographic_composition,behavior_description\"\n",
    "header=\"filename,species_common_name,individual_count,demographic_composition\"\n",
    "output_filename=\"video_analysis.csv\"\n",
    "add_content_path = True\n",
    "\n",
    "# Target detector in wildlife camera videos\n",
    "# content_folder = \"/home/george/codes/pyplayground/data/test/wildcam/Klelund_0113_211011_211124\"\n",
    "# # content_folder = \"/home/george/codes/pyplayground/data/test/wildcam/Klelund_0244_220530_220616\"\n",
    "# # content_folder = \"/home/george/codes/pyplayground/data/test/wildcam/Tofte_0052_230524_230531\"\n",
    "# system_prompt=\"\"\"\n",
    "# You are a wildlife video classifier. Analyze the video and respond with a single word based on these rules:\n",
    "\n",
    "# *   `target`: If any non-human animal is present.\n",
    "# *   `empty`: If no animals, humans, or vehicles are present.\n",
    "# *   `human`: If any human or vehicle is present.\n",
    "# \"\"\"\n",
    "# content_type=\"video\"\n",
    "# header=\"filename,animal_present\"\n",
    "# output_filename=\"is_there_a_target.csv\"\n",
    "# add_content_path = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859f588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": f\"file://{join(content_folder, p)}\"}\n",
    "        ],\n",
    "    }] for p in sorted(listdir(content_folder))\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "texts = [\n",
    "    processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n",
    "    for msg in messages\n",
    "]\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=texts,\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Batch Inference\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_texts = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a22765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt a bit the note taker\n",
    "# system_prompt = note_taker\n",
    "\n",
    "output_texts = []\n",
    "\n",
    "# Sort image paths\n",
    "content_paths = sorted(listdir(content_folder))\n",
    "\n",
    "def is_image_or_video(file_path):\n",
    "    # Check if it's an image\n",
    "    try:\n",
    "        with Image.open(file_path) as img:\n",
    "            img.verify()\n",
    "        return True\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Check if it's a video\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(file_path)\n",
    "        if cap.isOpened():\n",
    "            cap.release()\n",
    "            return True\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return False\n",
    "\n",
    "# Process images one at a time\n",
    "for content_name in content_paths:\n",
    "    content_path = join(content_folder, content_name)\n",
    "    if not is_image_or_video(content_path):\n",
    "        print(f\"Path {content_path} is not an image.\")\n",
    "        continue\n",
    "\n",
    "    # Construct message\n",
    "    messages = [\n",
    "        # {\n",
    "        #     \"role\": \"system\",\n",
    "        #     \"content\": system_prompt,\n",
    "        # },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": content_type, content_type: f\"file://{content_path}\"},\n",
    "                {\"type\": \"text\", \"text\": system_prompt},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Format for model input\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info([messages])\n",
    "\n",
    "    inputs = processor(\n",
    "        text=text,\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "\n",
    "    # Inference\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "    generated_ids_trimmed = generated_ids[0][len(inputs.input_ids[0]) :]  # Single sample\n",
    "    output_text = processor.decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    if add_content_path:\n",
    "        output_text = content_path + \",\" + output_text\n",
    "    output_texts.append(output_text)\n",
    "\n",
    "print(\"Transcription done. Uniformization...\")\n",
    "\n",
    "# # Last pass to uniformized the text\n",
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"user\",\n",
    "#         \"content\": [\n",
    "#             {\"type\": \"text\", \"text\": f\"Rewrite the text by following the rules:\\n\\nText:\\n{output_texts}\\n\\nRules:\\n{system_prompt}\"},\n",
    "#         ],\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# # Preparation for inference\n",
    "# text = processor.apply_chat_template(\n",
    "#     messages, tokenize=False, add_generation_prompt=True\n",
    "# )\n",
    "# inputs = processor(\n",
    "#     text=[text],\n",
    "#     padding=True,\n",
    "#     return_tensors=\"pt\",\n",
    "# )\n",
    "# inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# # Inference: Generation of the output\n",
    "# generated_ids = model.generate(**inputs, max_new_tokens=2048)\n",
    "# generated_ids_trimmed = [\n",
    "#     out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "# ]\n",
    "# output_text = processor.batch_decode(\n",
    "#     generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "# )\n",
    "\n",
    "# print(\"Done uniformizing. Saving...\")\n",
    "\n",
    "\n",
    "# Done\n",
    "transcribtion_path = join(content_folder, output_filename)\n",
    "\n",
    "with open(transcribtion_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    if header is not None and len(header) > 0:\n",
    "        f.write(header+\"\\n\")\n",
    "    for text in output_texts:\n",
    "        f.write(text + \"\\n\")\n",
    "\n",
    "print(f\"Transcribtion stored in {transcribtion_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74c53c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribtion_path = join(image_folder, \"transcriptions.txt\")\n",
    "with open(transcribtion_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for path, text in zip(image_paths, output_texts):\n",
    "        f.write(path + \",\" + text + \"\\n\")\n",
    "\n",
    "print(f\"Transcribtion stored in {transcribtion_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ab2a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": reformatting_system,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": f\"{output_texts}\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=2048)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8c965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef40056d",
   "metadata": {},
   "source": [
    "Application to videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f56a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wildlife camera detector\n",
    "video_folder = \"/home/george/codes/pyplayground/data/test/wildcam/images\"\n",
    "system_prompt=\"\"\"\n",
    "You are an automated wildlife observation analysis system. Your task is to analyze the provided video and extract key information into a single, specific CSV-formatted line.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Analyze the entire video for animal presence, behavior, and environmental context.\n",
    "2.  Extract all metadata from the text overlay.\n",
    "3.  Your entire output must be a single line of text containing 13 comma-separated values in the exact order specified below.\n",
    "4.  Do not output headers, explanations, or any text other than the single CSV line.\n",
    "5.  If a value cannot be determined, use the specified default.\n",
    "\n",
    "**CSV Field Order and Content:**\n",
    "1.  **`video_filename`**: Use the placeholder `[VIDEO_FILENAME]`.\n",
    "2.  **`animal_present`**: `TRUE` if an animal is visible, otherwise `FALSE`.\n",
    "3.  **`species_common_name`**: Common name. Default: `N/A`.\n",
    "4.  **`species_scientific_name`**: Scientific name. Default: `N/A`.\n",
    "5.  **`individual_count`**: Integer count of the primary species. Default: `0`.\n",
    "6.  **`sex`**: `Female`, `Male`, or `Unknown`. Default: `N/A`.\n",
    "7.  **`behavior_description`**: Short, neutral keywords (e.g., `walking, foraging, observing`). Default: `N/A`.\n",
    "8.  **`habitat_description`**: Short, neutral keywords (e.g., `pine forest, grassland, riverbed`).\n",
    "9.  **`camera_id`**: Alphanumeric ID from the overlay.\n",
    "10. **`timestamp_utc`**: Full timestamp from overlay, converted to `YYYY-MM-DDTHH:MM:SSZ` format.\n",
    "11. **`temperature_celsius`**: Numeric temperature value.\n",
    "12. **`pressure`**: Numeric pressure value.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f520ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"video\", \"video\": f\"file://{join(video_folder, p)}\"}\n",
    "        ],\n",
    "    }] for p in sorted(listdir(video_folder))\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "texts = [\n",
    "    processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n",
    "    for msg in messages\n",
    "]\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=texts,\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Batch Inference\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_texts = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28adfe2",
   "metadata": {},
   "source": [
    "Try to design my own chatting bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5458b477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextIteratorStreamer\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import os\n",
    "from threading import Thread\n",
    "\n",
    "# --- The Main Chat Interface Class for API Usage ---\n",
    "class QwenVLChat:\n",
    "    \"\"\"\n",
    "    A versatile chat class for Qwen2.5-VL that supports both blocking (API-style)\n",
    "    and streaming (generator-style) responses.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, processor, system_prompt=None):\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.history = []\n",
    "        self.system_msg=None\n",
    "        if system_prompt is not None and isinstance(system_prompt, str):\n",
    "            self.system_msg={\"role\": \"system\", \"content\": system_prompt}\n",
    "        self.device = model.device\n",
    "        print(\"QwenVLChat initialized. Ready to chat.\")\n",
    "\n",
    "    def _prepare_inputs_history(self, prompt: str = None, image_path: str = None):\n",
    "        \"\"\"Internal method to prepare model inputs and update history.\"\"\"\n",
    "        # Eventually add system prompt\n",
    "        msg = []\n",
    "        if self.system_msg is not None:\n",
    "            msg.append(self.system_msg)\n",
    "\n",
    "        # Create the user message content\n",
    "        content = []\n",
    "        if image_path: # TODO: adapt to URLs\n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"Warning: Image path not found: {image_path}. Ignoring image.\")\n",
    "            else:\n",
    "                content.append({\"type\": \"image\", \"image\": f\"file://{image_path}\"})\n",
    "        if prompt is not None:\n",
    "            content.append({\"type\": \"text\", \"text\": prompt})\n",
    "\n",
    "        # Append the new user message to the conversation history\n",
    "        msg.append({\"role\": \"user\", \"content\": content})\n",
    "\n",
    "        # Append to the history\n",
    "        self.history.append(msg)\n",
    "\n",
    "        # Process the entire history for the model\n",
    "        texts = [self.processor.apply_chat_template(\n",
    "            msg, tokenize=False, add_generation_prompt=True\n",
    "        ) for msg in self.history]\n",
    "        image_inputs, video_inputs = process_vision_info(self.history)\n",
    "        \n",
    "        inputs = self.processor(\n",
    "            text=texts,\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.device)\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    def _prepare_inputs(self, prompt: str = None, image_path: str = None):\n",
    "        \"\"\"Internal method to prepare model inputs and update history.\"\"\"\n",
    "        # Eventually add system prompt\n",
    "        msg = []\n",
    "        if self.system_msg is not None:\n",
    "            msg.append(self.system_msg)\n",
    "\n",
    "        # Create the user message content\n",
    "        content = []\n",
    "        if image_path: # TODO: adapt to URLs\n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"Warning: Image path not found: {image_path}. Ignoring image.\")\n",
    "            else:\n",
    "                content.append({\"type\": \"image\", \"image\": f\"file://{image_path}\"})\n",
    "        if prompt is not None:\n",
    "            content.append({\"type\": \"text\", \"text\": prompt})\n",
    "\n",
    "        # Append the new user message to the conversation history\n",
    "        msg.append({\"role\": \"user\", \"content\": content})\n",
    "\n",
    "        # Process the entire history for the model\n",
    "        texts = [self.processor.apply_chat_template(\n",
    "            msg, tokenize=False, add_generation_prompt=True\n",
    "        )]\n",
    "        image_inputs, video_inputs = process_vision_info(msg)\n",
    "        \n",
    "        inputs = self.processor(\n",
    "            text=texts,\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.device)\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "    def generate_response(self, prompt: str = None, image_path: str = None, generation_args: dict = None) -> str:\n",
    "        \"\"\"\n",
    "        Generates a complete, blocking response. This is the most robust method\n",
    "        and handles all cases, including image-only inputs.\n",
    "        It is not limited by token count and generates until the model stops.\n",
    "        \n",
    "        Args:\n",
    "            prompt (str, optional): The text prompt.\n",
    "            image_path (str, optional): The path to the image file.\n",
    "            generation_args (dict, optional): Override generation parameters.\n",
    "        \n",
    "        Returns:\n",
    "            str: The full text response from the model.\n",
    "        \"\"\"\n",
    "        print(self.history)\n",
    "        inputs = self._prepare_inputs(prompt, image_path)\n",
    "\n",
    "        # Set a high max_new_tokens to allow for long responses, mimicking \"unlimited\" generation.\n",
    "        # The model will stop on its own when it generates an EOS token.\n",
    "        gen_kwargs = {\n",
    "            \"max_new_tokens\": 4096,\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            **(generation_args or {})\n",
    "        }\n",
    "\n",
    "        # Generate the token IDs\n",
    "        generated_ids = self.model.generate(**inputs, **gen_kwargs)\n",
    "        # generated_ids = self.model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "        # Trim the input tokens from the generated output\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "\n",
    "        # Decode the trimmed token IDs to get the final text\n",
    "        # We access [0] because batch_decode returns a list\n",
    "        response_text = processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "        \n",
    "        # Now that generation is successful, update the history\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": response_text})\n",
    "        \n",
    "        return response_text\n",
    "\n",
    "    def chat(self, prompt: str = None, image_path: str = None, generation_args: dict = None) -> str:\n",
    "        \"\"\"\n",
    "        A simple, blocking chat method.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The text prompt from the user.\n",
    "            image_path (str, optional): The path to the image file.\n",
    "            generation_args (dict, optional): Arguments for model.generate().\n",
    "\n",
    "        Returns:\n",
    "            str: The complete response from the model.\n",
    "        \"\"\"\n",
    "        # Simply call the streaming method and concatenate the results.\n",
    "        assert prompt is not None or image_path is not None, \"Please provide either a prompt or an image path.\"\n",
    "        response_tokens = []\n",
    "        for token in self.chat_stream(prompt, image_path, generation_args):\n",
    "            response_tokens.append(token)\n",
    "            \n",
    "        return \"\".join(response_tokens)\n",
    "\n",
    "    def chat_stream(self, prompt: str = None, image_path: str = None, generation_args: dict = None):\n",
    "        \"\"\"\n",
    "        A streaming chat method that yields tokens as they are generated.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The text prompt from the user.\n",
    "            image_path (str, optional): The path to the image file.\n",
    "            generation_args (dict, optional): Arguments for model.generate().\n",
    "\n",
    "        Yields:\n",
    "            str: The next token generated by the model.\n",
    "        \"\"\"\n",
    "        assert prompt is not None or image_path is not None, \"Please provide either a prompt or an image path.\"\n",
    "        inputs = self._prepare_inputs(prompt, image_path)\n",
    "        streamer = TextIteratorStreamer(self.processor.tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "        # Default generation arguments\n",
    "        gen_args = {\n",
    "            \"max_new_tokens\": 2048,\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            **inputs,\n",
    "            \"streamer\": streamer,\n",
    "        }\n",
    "        if generation_args:\n",
    "            gen_args.update(generation_args)\n",
    "        \n",
    "        # Run generation in a separate thread to not block the main thread\n",
    "        thread = Thread(target=self.model.generate, kwargs=gen_args)\n",
    "        thread.start()\n",
    "\n",
    "        # Yield tokens as they become available and build the full response\n",
    "        full_response = []\n",
    "        for new_token in streamer:\n",
    "            yield new_token\n",
    "            full_response.append(new_token)\n",
    "        \n",
    "        thread.join() # Ensure the generation thread is finished\n",
    "\n",
    "        # After streaming is complete, add the full assistant response to history\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": \"\".join(full_response)})\n",
    "\n",
    "    def get_history(self):\n",
    "        \"\"\"Returns the current conversation history.\"\"\"\n",
    "        return self.history\n",
    "\n",
    "    def clear_history(self):\n",
    "        \"\"\"Resets the conversation history.\"\"\"\n",
    "        self.history = []\n",
    "        print(\"Conversation history cleared.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c19582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize Chatbot ---\n",
    "chatbot = QwenVLChat(model, processor)\n",
    "\n",
    "# --- Example API Interaction ---\n",
    "image_file = \"data/scan.jpg\" # IMPORTANT: Make sure this file exists\n",
    "if not os.path.exists(image_file):\n",
    "    print(f\"\\nERROR: The image '{image_file}' was not found.\")\n",
    "    print(\"Please download or create a sample image and place it in the 'data' directory.\")\n",
    "    exit()\n",
    "\n",
    "# --- DEMO 1: Using the simple, blocking `chat()` API ---\n",
    "print(\"\\n--- DEMO 1: Simple Blocking API Call ---\")\n",
    "prompt1 = \"Transcribe the handwritten text in this picture.\"\n",
    "print(f\"User: {prompt1}\")\n",
    "\n",
    "# The call blocks until the full response is ready\n",
    "response1 = chatbot.chat(prompt=prompt1, image_path=image_file)\n",
    "\n",
    "print(f\"Assistant: {response1}\")\n",
    "print(\"----------------------------------------\\n\")\n",
    "\n",
    "# --- DEMO 2: Using the `chat_stream()` API for real-time output ---\n",
    "print(\"--- DEMO 2: Streaming API Call ---\")\n",
    "prompt2 = \"Based on the text you transcribed, please write a short summary.\"\n",
    "print(f\"User: {prompt2}\")\n",
    "print(\"Assistant (streaming): \", end=\"\", flush=True)\n",
    "\n",
    "# The call returns a generator immediately. We loop through it.\n",
    "full_response_streamed = \"\"\n",
    "for token in chatbot.chat_stream(prompt=prompt2):\n",
    "    print(token, end=\"\", flush=True)\n",
    "    full_response_streamed += token\n",
    "print(\"\\n----------------------------------------\\n\")\n",
    "\n",
    "# --- DEMO 3: Verify conversation history ---\n",
    "print(\"--- DEMO 3: Verifying Conversation History ---\")\n",
    "print(\"The conversation history is maintained across both blocking and streaming calls.\")\n",
    "\n",
    "# A final blocking call that relies on the context from the previous turns\n",
    "prompt3 = \"What was the very first thing I asked you to do?\"\n",
    "print(f\"User: {prompt3}\")\n",
    "response3 = chatbot.chat(prompt=prompt3)\n",
    "print(f\"Assistant: {response3}\")\n",
    "print(\"----------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a941ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\"You are a classification model. \"\n",
    "                \"You only gives two possible outputs: \"\n",
    "                \"'Yes' if the input image contains an insect visiting the \"\n",
    "                \"flower. 'No' if the input image does not contain an insect \"\n",
    "                \"or if the insect in the image is not on the flower.\")\n",
    "insect_detector = QwenVLChat(model, processor, system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4e67dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file = \"data/insect/WSCT9000.JPG\"\n",
    "response1 = insect_detector.generate_response(image_path=image_file)\n",
    "\n",
    "print(f\"Assistant: {response1}\")\n",
    "print(\"----------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfa2b4f",
   "metadata": {},
   "source": [
    "OOM below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c570d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\"image-text-to-text\", model=\"Qwen/Qwen2.5-VL-7B-Instruct\", torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"cuda:0\", use_fast=True)\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n",
    "            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "pipe(text=messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b482ce0",
   "metadata": {},
   "source": [
    "# InternVL3\n",
    "\n",
    "Source: https://huggingface.co/OpenGVLab/InternVL3-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01605e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "path = \"OpenGVLab/InternVL3-8B\"\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attn=True,\n",
    "    trust_remote_code=True).eval().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7693b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "def split_model(model_name, world_size=None):\n",
    "    device_map = {}\n",
    "    world_size = torch.cuda.device_count() if world_size is None else world_size\n",
    "    config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "    num_layers = config.llm_config.num_hidden_layers\n",
    "    # Since the first GPU will be used for ViT, treat it as half a GPU.\n",
    "    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
    "    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n",
    "    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n",
    "    layer_cnt = 0\n",
    "    for i, num_layer in enumerate(num_layers_per_gpu):\n",
    "        for j in range(num_layer):\n",
    "            device_map[f'language_model.model.layers.{layer_cnt}'] = i\n",
    "            layer_cnt += 1\n",
    "    device_map['vision_model'] = 0\n",
    "    device_map['mlp1'] = 0\n",
    "    device_map['language_model.model.tok_embeddings'] = 0\n",
    "    device_map['language_model.model.embed_tokens'] = 0\n",
    "    device_map['language_model.output'] = 0\n",
    "    device_map['language_model.model.norm'] = 0\n",
    "    device_map['language_model.model.rotary_emb'] = 0\n",
    "    device_map['language_model.lm_head'] = 0\n",
    "    device_map[f'language_model.model.layers.{num_layers - 1}'] = 0\n",
    "\n",
    "    return device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a2e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5122afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the max number of tiles in `max_num`\n",
    "pixel_values = load_image('data/what.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=True)\n",
    "\n",
    "# pure-text conversation (纯文本对话)\n",
    "question = 'Hello, who are you?'\n",
    "response, history = model.chat(tokenizer, None, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb790710",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '<image>\\nPlease describe the image shortly.'\n",
    "response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "print(f'User: {question}\\nAssistant: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb85801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values = load_image('data/scan.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "question = '<image>\\nPlease transcribe the handwritten text in this picture.'\n",
    "response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "print(f'User: {question}\\nAssistant: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30232af5",
   "metadata": {},
   "source": [
    "## Qwen3 on wildcams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54350a52-7afc-4958-ab8d-69c622545dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_dir = Path(\"/work/datasets/wildcams/testData\")\n",
    "csv_pth = Path(\"/work/datasets/wildcams/WildCam_testDataSet.csv\")\n",
    "trg_pth = Path(\"/work/datasets/wildcams/deepfaune_1.4_ids.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f93e79ed-eda0-406f-8e20-d0d14b93d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_linux_format = lambda  x : x.str.replace(r'\\\\', '/', regex=True)\n",
    "\n",
    "df=pd.read_csv(csv_pth)\n",
    "df['filepath'] = to_linux_format(df['filepath'])\n",
    "df_trg=pd.read_csv(trg_pth)\n",
    "df_trg['filename'] = to_linux_format(df_trg['filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af7ab0a6-9952-4ed2-8587-f1589e3f66c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_outputs = list(df_trg['prediction'].unique())\n",
    "possible_types = list(df['type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f90674b-2dcd-459f-9863-6b390b75e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_pth=vid_dir / Path(*Path(df['filepath'].iloc[0]).parts[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a3c15b6-e4ef-46fd-9707-7eb4a9c04736",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,r in df.iterrows():\n",
    "    vid_pth=vid_dir / Path(*Path(df['filepath'].iloc[0]).parts[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d92e3ef-b2c6-4395-8220-20f2c8a4fdef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>filepath</th>\n",
       "      <th>video_name</th>\n",
       "      <th>area</th>\n",
       "      <th>series</th>\n",
       "      <th>TimeStampCET</th>\n",
       "      <th>year</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>species</th>\n",
       "      <th>count</th>\n",
       "      <th>type</th>\n",
       "      <th>certainty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Klelund_0001_200907_200911_file_20200909_040300</td>\n",
       "      <td>D:/NHMA/LUDA/Klelund_0001/Klelund_0001_200907_...</td>\n",
       "      <td>Rec_20200909_020311_151_M</td>\n",
       "      <td>Klelund</td>\n",
       "      <td>Klelund_0001_200907_200911</td>\n",
       "      <td>2020-09-09 04:03:22</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-09-09</td>\n",
       "      <td>04:03:00</td>\n",
       "      <td>fox</td>\n",
       "      <td>1</td>\n",
       "      <td>adult (unsp. Sex)</td>\n",
       "      <td>Helt sikker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Klelund_0001_200907_200911_file_20200910_123500</td>\n",
       "      <td>D:/NHMA/LUDA/Klelund_0001/Klelund_0001_200907_...</td>\n",
       "      <td>Rec_20200910_103531_151_M</td>\n",
       "      <td>Klelund</td>\n",
       "      <td>Klelund_0001_200907_200911</td>\n",
       "      <td>2020-09-10 12:35:42</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-09-10</td>\n",
       "      <td>12:35:00</td>\n",
       "      <td>red deer</td>\n",
       "      <td>1</td>\n",
       "      <td>female adult</td>\n",
       "      <td>Helt sikker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Klelund_0002_200907_200928_file_20200912_183600</td>\n",
       "      <td>D:/NHMA/LUDA/Klelund_0002/Klelund_0002_200907_...</td>\n",
       "      <td>Rec_20200912_163616_151_M</td>\n",
       "      <td>Klelund</td>\n",
       "      <td>Klelund_0002_200907_200928</td>\n",
       "      <td>2020-09-12 18:36:29</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-09-12</td>\n",
       "      <td>18:36:00</td>\n",
       "      <td>roe deer</td>\n",
       "      <td>1</td>\n",
       "      <td>male adult</td>\n",
       "      <td>Helt sikker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Klelund_0002_200907_200928_file_20200913_014700</td>\n",
       "      <td>D:/NHMA/LUDA/Klelund_0002/Klelund_0002_200907_...</td>\n",
       "      <td>Rec_20200912_234706_151_M</td>\n",
       "      <td>Klelund</td>\n",
       "      <td>Klelund_0002_200907_200928</td>\n",
       "      <td>2020-09-13 01:47:17</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-09-13</td>\n",
       "      <td>01:47:00</td>\n",
       "      <td>fox</td>\n",
       "      <td>1</td>\n",
       "      <td>adult (unsp. Sex)</td>\n",
       "      <td>Mulig/overvejende sandsynlig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Klelund_0002_200907_200928_file_20200916_021000</td>\n",
       "      <td>D:/NHMA/LUDA/Klelund_0002/Klelund_0002_200907_...</td>\n",
       "      <td>Rec_20200916_001019_151_M</td>\n",
       "      <td>Klelund</td>\n",
       "      <td>Klelund_0002_200907_200928</td>\n",
       "      <td>2020-09-16 02:10:30</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-09-16</td>\n",
       "      <td>02:10:00</td>\n",
       "      <td>fox</td>\n",
       "      <td>1</td>\n",
       "      <td>adult (unsp. Sex)</td>\n",
       "      <td>Helt sikker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8692</th>\n",
       "      <td>Klelund_0279_230824_231005_file_20230826_121900</td>\n",
       "      <td>D:/NHMA/LUDA/Klelund_0279/Klelund_0279_230824_...</td>\n",
       "      <td>Rec_20230826_101921_151_M</td>\n",
       "      <td>Klelund</td>\n",
       "      <td>Klelund_0279_230824_231005</td>\n",
       "      <td>2023-08-26 12:19:37</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-08-26</td>\n",
       "      <td>12:19:36</td>\n",
       "      <td>roe deer</td>\n",
       "      <td>2</td>\n",
       "      <td>male adult</td>\n",
       "      <td>Helt sikker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8693</th>\n",
       "      <td>Klelund_0279_230824_231005_file_20230826_205200</td>\n",
       "      <td>D:/NHMA/LUDA/Klelund_0279/Klelund_0279_230824_...</td>\n",
       "      <td>Rec_20230826_185236_151_M</td>\n",
       "      <td>Klelund</td>\n",
       "      <td>Klelund_0279_230824_231005</td>\n",
       "      <td>2023-08-26 20:52:48</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-08-26</td>\n",
       "      <td>20:52:48</td>\n",
       "      <td>roe deer</td>\n",
       "      <td>1</td>\n",
       "      <td>male adult</td>\n",
       "      <td>Helt sikker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8694</th>\n",
       "      <td>Klelund_0279_230824_231005_file_20230909_023500</td>\n",
       "      <td>D:/NHMA/LUDA/Klelund_0279/Klelund_0279_230824_...</td>\n",
       "      <td>Rec_20230909_003529_151_M</td>\n",
       "      <td>Klelund</td>\n",
       "      <td>Klelund_0279_230824_231005</td>\n",
       "      <td>2023-09-09 02:35:42</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-09-09</td>\n",
       "      <td>02:35:42</td>\n",
       "      <td>red deer</td>\n",
       "      <td>3</td>\n",
       "      <td>female adult</td>\n",
       "      <td>Helt sikker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8695</th>\n",
       "      <td>Klelund_0279_230824_231005_file_20230913_063900</td>\n",
       "      <td>D:/NHMA/LUDA/Klelund_0279/Klelund_0279_230824_...</td>\n",
       "      <td>Rec_20230913_043922_151_M</td>\n",
       "      <td>Klelund</td>\n",
       "      <td>Klelund_0279_230824_231005</td>\n",
       "      <td>2023-09-13 06:39:34</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-09-13</td>\n",
       "      <td>06:39:34</td>\n",
       "      <td>roe deer</td>\n",
       "      <td>1</td>\n",
       "      <td>female adult</td>\n",
       "      <td>Helt sikker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8696</th>\n",
       "      <td>Klelund_0279_230824_231005_file_20230915_150700</td>\n",
       "      <td>D:/NHMA/LUDA/Klelund_0279/Klelund_0279_230824_...</td>\n",
       "      <td>Rec_20230915_130724_151_M</td>\n",
       "      <td>Klelund</td>\n",
       "      <td>Klelund_0279_230824_231005</td>\n",
       "      <td>2023-09-15 15:07:36</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-09-15</td>\n",
       "      <td>15:07:36</td>\n",
       "      <td>roe deer</td>\n",
       "      <td>1</td>\n",
       "      <td>female adult</td>\n",
       "      <td>Helt sikker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8697 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             filename  \\\n",
       "0     Klelund_0001_200907_200911_file_20200909_040300   \n",
       "1     Klelund_0001_200907_200911_file_20200910_123500   \n",
       "2     Klelund_0002_200907_200928_file_20200912_183600   \n",
       "3     Klelund_0002_200907_200928_file_20200913_014700   \n",
       "4     Klelund_0002_200907_200928_file_20200916_021000   \n",
       "...                                               ...   \n",
       "8692  Klelund_0279_230824_231005_file_20230826_121900   \n",
       "8693  Klelund_0279_230824_231005_file_20230826_205200   \n",
       "8694  Klelund_0279_230824_231005_file_20230909_023500   \n",
       "8695  Klelund_0279_230824_231005_file_20230913_063900   \n",
       "8696  Klelund_0279_230824_231005_file_20230915_150700   \n",
       "\n",
       "                                               filepath  \\\n",
       "0     D:/NHMA/LUDA/Klelund_0001/Klelund_0001_200907_...   \n",
       "1     D:/NHMA/LUDA/Klelund_0001/Klelund_0001_200907_...   \n",
       "2     D:/NHMA/LUDA/Klelund_0002/Klelund_0002_200907_...   \n",
       "3     D:/NHMA/LUDA/Klelund_0002/Klelund_0002_200907_...   \n",
       "4     D:/NHMA/LUDA/Klelund_0002/Klelund_0002_200907_...   \n",
       "...                                                 ...   \n",
       "8692  D:/NHMA/LUDA/Klelund_0279/Klelund_0279_230824_...   \n",
       "8693  D:/NHMA/LUDA/Klelund_0279/Klelund_0279_230824_...   \n",
       "8694  D:/NHMA/LUDA/Klelund_0279/Klelund_0279_230824_...   \n",
       "8695  D:/NHMA/LUDA/Klelund_0279/Klelund_0279_230824_...   \n",
       "8696  D:/NHMA/LUDA/Klelund_0279/Klelund_0279_230824_...   \n",
       "\n",
       "                     video_name     area                      series  \\\n",
       "0     Rec_20200909_020311_151_M  Klelund  Klelund_0001_200907_200911   \n",
       "1     Rec_20200910_103531_151_M  Klelund  Klelund_0001_200907_200911   \n",
       "2     Rec_20200912_163616_151_M  Klelund  Klelund_0002_200907_200928   \n",
       "3     Rec_20200912_234706_151_M  Klelund  Klelund_0002_200907_200928   \n",
       "4     Rec_20200916_001019_151_M  Klelund  Klelund_0002_200907_200928   \n",
       "...                         ...      ...                         ...   \n",
       "8692  Rec_20230826_101921_151_M  Klelund  Klelund_0279_230824_231005   \n",
       "8693  Rec_20230826_185236_151_M  Klelund  Klelund_0279_230824_231005   \n",
       "8694  Rec_20230909_003529_151_M  Klelund  Klelund_0279_230824_231005   \n",
       "8695  Rec_20230913_043922_151_M  Klelund  Klelund_0279_230824_231005   \n",
       "8696  Rec_20230915_130724_151_M  Klelund  Klelund_0279_230824_231005   \n",
       "\n",
       "             TimeStampCET  year        date      time   species  count  \\\n",
       "0     2020-09-09 04:03:22  2020  2020-09-09  04:03:00       fox      1   \n",
       "1     2020-09-10 12:35:42  2020  2020-09-10  12:35:00  red deer      1   \n",
       "2     2020-09-12 18:36:29  2020  2020-09-12  18:36:00  roe deer      1   \n",
       "3     2020-09-13 01:47:17  2020  2020-09-13  01:47:00       fox      1   \n",
       "4     2020-09-16 02:10:30  2020  2020-09-16  02:10:00       fox      1   \n",
       "...                   ...   ...         ...       ...       ...    ...   \n",
       "8692  2023-08-26 12:19:37  2023  2023-08-26  12:19:36  roe deer      2   \n",
       "8693  2023-08-26 20:52:48  2023  2023-08-26  20:52:48  roe deer      1   \n",
       "8694  2023-09-09 02:35:42  2023  2023-09-09  02:35:42  red deer      3   \n",
       "8695  2023-09-13 06:39:34  2023  2023-09-13  06:39:34  roe deer      1   \n",
       "8696  2023-09-15 15:07:36  2023  2023-09-15  15:07:36  roe deer      1   \n",
       "\n",
       "                   type                     certainty  \n",
       "0     adult (unsp. Sex)                   Helt sikker  \n",
       "1          female adult                   Helt sikker  \n",
       "2            male adult                   Helt sikker  \n",
       "3     adult (unsp. Sex)  Mulig/overvejende sandsynlig  \n",
       "4     adult (unsp. Sex)                   Helt sikker  \n",
       "...                 ...                           ...  \n",
       "8692         male adult                   Helt sikker  \n",
       "8693         male adult                   Helt sikker  \n",
       "8694       female adult                   Helt sikker  \n",
       "8695       female adult                   Helt sikker  \n",
       "8696       female adult                   Helt sikker  \n",
       "\n",
       "[8697 rows x 13 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "087d1517-c07d-4e3f-9ec2-737a0bd676ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image_or_video(file_path):\n",
    "    # Check if it's an image\n",
    "    try:\n",
    "        with Image.open(file_path) as img:\n",
    "            img.verify()\n",
    "        return True\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Check if it's a video\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(file_path)\n",
    "        if cap.isOpened():\n",
    "            cap.release()\n",
    "            return True\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "system_prompt=f\"\"\"\n",
    "You are an expert wildlife analyst. Your task is to analyze the provided video and generate a single line of CSV data describing the primary animal species present. \n",
    "\n",
    "Use the following header and rules for each field:\n",
    "`prediction,count,type,score`\n",
    "\n",
    "1.  `prediction`: The common name of the primary species. One of {possible_outputs}\n",
    "2.  `count`: The total number of individuals of that species.\n",
    "3.  `type`: Describe the primary subject or group. Use one: {possible_types}.\n",
    "4.  `score`: An integer between 0 and 99 indicating the level of confidence of the prediction.\n",
    "\n",
    "Here is an example of valid output:\n",
    "red deer,1,female,99\n",
    "\"\"\"\n",
    "\n",
    "def qwen_pred(content_path, system_prompt, content_type=\"video\", add_content_path=True):\n",
    "    if not is_image_or_video(content_path):\n",
    "        print(f\"Path {content_path} is not an image.\")\n",
    "        return \"\"\n",
    "\n",
    "    # Construct message\n",
    "    messages = [\n",
    "        # {\n",
    "        #     \"role\": \"system\",\n",
    "        #     \"content\": system_prompt,\n",
    "        # },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                # {\"type\": content_type, content_type: f\"file://{content_path}\"},\n",
    "                {\"type\": content_type, content_type: str(content_path)},\n",
    "                {\"type\": \"text\", \"text\": system_prompt},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Format for model input\n",
    "    # text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    # image_inputs, video_inputs = process_vision_info([messages])\n",
    "\n",
    "    # inputs = processor(\n",
    "    #     text=text,\n",
    "    #     images=image_inputs,\n",
    "    #     videos=video_inputs,\n",
    "    #     padding=True,\n",
    "    #     return_tensors=\"pt\",\n",
    "    # )\n",
    "    # inputs = inputs.to(\"cuda\")\n",
    "\n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "\n",
    "    # Inference\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "    generated_ids_trimmed = generated_ids[0][len(inputs.input_ids[0]) :]  # Single sample\n",
    "    output_text = processor.decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    if add_content_path:\n",
    "        output_text = str(content_path) + \",\" + output_text\n",
    "    return str(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a49c983a-0aed-4064-baa0-4dca3a55dec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/work/datasets/wildcams/testData/LUDA/Klelund_0001/Klelund_0001_200907_200911/Rec_20200909_020311_151_M.mp4')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid_pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90dc9925-742e-47af-a13e-2a302ef9ce93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/datasets/wildcams/testData/LUDA/Klelund_0001/Klelund_0001_200907_200911/Rec_20200909_020311_151_M.mp4,fox,1,adult (unsp. Sex),95\n",
      "/work/datasets/wildcams/testData/LUDA/Klelund_0001/Klelund_0001_200907_200911/Rec_20200910_103531_151_M.mp4,roe deer,1,male adult,95\n",
      "/work/datasets/wildcams/testData/LUDA/Klelund_0002/Klelund_0002_200907_200928/Rec_20200912_163616_151_M.mp4,roe deer,1,adult (unsp. Sex),95\n",
      "/work/datasets/wildcams/testData/LUDA/Klelund_0002/Klelund_0002_200907_200928/Rec_20200912_234706_151_M.mp4,fox,1,adult (unsp. Sex),85\n",
      "/work/datasets/wildcams/testData/LUDA/Klelund_0002/Klelund_0002_200907_200928/Rec_20200916_001019_151_M.mp4,empty,0,empty,99\n",
      "/work/datasets/wildcams/testData/LUDA/Klelund_0002/Klelund_0002_200907_200928/Rec_20200917_222512_151_M.mp4,fox,1,adult (unsp. Sex),95\n",
      "/work/datasets/wildcams/testData/LUDA/Klelund_0002/Klelund_0002_200907_200928/Rec_20200926_221927_151_M.mp4,fox,1,adult (unsp. Sex),95\n",
      "/work/datasets/wildcams/testData/LUDA/Klelund_0003/Klelund_0003_200929_201026/Rec_20201003_013438_151_M.mp4,lagomorph,1,adult (unsp. Sex),90\n",
      "/work/datasets/wildcams/testData/LUDA/Klelund_0003/Klelund_0003_200929_201026/Rec_20201003_212329_151_M.mp4,roe deer,1,adult (unsp. Sex),95\n",
      "/work/datasets/wildcams/testData/LUDA/Klelund_0003/Klelund_0003_200929_201026/Rec_20201006_053607_151_M.mp4,red deer,1,male adult,85\n",
      "/work/datasets/wildcams/testData/LUDA/Klelund_0003/Klelund_0003_200929_201026/Rec_20201009_132507_151_M.mp4,red deer,7,adult (unsp. Sex),95\n"
     ]
    }
   ],
   "source": [
    "for i,r in df.iterrows():\n",
    "    vid_pth=vid_dir / Path(*Path(r['filepath']).parts[2:])\n",
    "    print(qwen_pred(vid_pth, system_prompt))\n",
    "    if i==10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554adc17-44bd-4938-82ec-8cf76154b22e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
